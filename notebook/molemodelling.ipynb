{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moledetection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    6705\n",
      "4    1113\n",
      "2    1099\n",
      "1     514\n",
      "0     327\n",
      "6     142\n",
      "3     115\n",
      "Name: label, dtype: int64\n",
      "@$$$$$$$$$ (197, 8) (65, 8) (65, 8)\n",
      "@$$$-----$ (500, 8) (197, 8) (65, 8) (65, 8)\n",
      "#prepare train set\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n",
      "@$$$--x---$ (2000, 64, 64, 3) (3500, 64, 64, 3) (2000, 64, 64, 3) @@ (500, 8) (197, 8) (65, 8) (65, 8)\n",
      "@@@@@@@ 3500 2000 2000\n",
      "# start model selection\n",
      "INFO:tensorflow:Reloading Oracle from existing project .\\image_classifier\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\image_classifier\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Epoch 1/25\n",
      "32/32 [==============================] - 18s 552ms/step - loss: 1.2163 - accuracy: 0.6370\n",
      "Epoch 2/25\n",
      "32/32 [==============================] - 18s 557ms/step - loss: 1.0067 - accuracy: 0.6870\n",
      "Epoch 3/25\n",
      "32/32 [==============================] - 19s 584ms/step - loss: 0.9293 - accuracy: 0.6890\n",
      "Epoch 4/25\n",
      "32/32 [==============================] - 21s 664ms/step - loss: 0.9351 - accuracy: 0.6940\n",
      "Epoch 5/25\n",
      "32/32 [==============================] - 22s 672ms/step - loss: 0.9295 - accuracy: 0.6870\n",
      "Epoch 6/25\n",
      "32/32 [==============================] - 21s 665ms/step - loss: 0.9140 - accuracy: 0.6830\n",
      "Epoch 7/25\n",
      "32/32 [==============================] - 22s 676ms/step - loss: 0.8969 - accuracy: 0.6890\n",
      "Epoch 8/25\n",
      "32/32 [==============================] - 21s 666ms/step - loss: 0.8549 - accuracy: 0.6770\n",
      "Epoch 9/25\n",
      "32/32 [==============================] - 21s 663ms/step - loss: 0.8510 - accuracy: 0.6860\n",
      "Epoch 10/25\n",
      "32/32 [==============================] - 22s 672ms/step - loss: 0.8303 - accuracy: 0.6880\n",
      "Epoch 11/25\n",
      "32/32 [==============================] - 22s 682ms/step - loss: 0.8474 - accuracy: 0.6880\n",
      "Epoch 12/25\n",
      "32/32 [==============================] - 21s 667ms/step - loss: 0.8280 - accuracy: 0.6820\n",
      "Epoch 13/25\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.8242 - accuracy: 0.6930\n",
      "Epoch 14/25\n",
      "32/32 [==============================] - 22s 677ms/step - loss: 0.8148 - accuracy: 0.6910\n",
      "Epoch 15/25\n",
      "32/32 [==============================] - 22s 682ms/step - loss: 0.7911 - accuracy: 0.7030\n",
      "Epoch 16/25\n",
      "32/32 [==============================] - 21s 669ms/step - loss: 0.7695 - accuracy: 0.7050\n",
      "Epoch 17/25\n",
      "32/32 [==============================] - 22s 677ms/step - loss: 0.7751 - accuracy: 0.7050\n",
      "Epoch 18/25\n",
      "32/32 [==============================] - 21s 666ms/step - loss: 0.7609 - accuracy: 0.7070\n",
      "Epoch 19/25\n",
      "32/32 [==============================] - 21s 660ms/step - loss: 0.7694 - accuracy: 0.7200\n",
      "Epoch 20/25\n",
      "32/32 [==============================] - 22s 674ms/step - loss: 0.7602 - accuracy: 0.7080\n",
      "Epoch 21/25\n",
      "32/32 [==============================] - 21s 662ms/step - loss: 0.7427 - accuracy: 0.7260\n",
      "Epoch 22/25\n",
      "32/32 [==============================] - 21s 661ms/step - loss: 0.7382 - accuracy: 0.7170\n",
      "Epoch 23/25\n",
      "32/32 [==============================] - 22s 687ms/step - loss: 0.7756 - accuracy: 0.7060\n",
      "Epoch 24/25\n",
      "32/32 [==============================] - 22s 684ms/step - loss: 0.7010 - accuracy: 0.7380\n",
      "Epoch 25/25\n",
      "32/32 [==============================] - 22s 686ms/step - loss: 0.6998 - accuracy: 0.7320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\image_classifier\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\image_classifier\\best_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Evaluate the classifier on test data \n",
      "32/32 [==============================] - 5s 157ms/step - loss: 0.8546 - accuracy: 0.6760\n",
      "Accuracy =  67.59999990463257 %\n",
      "# get the final best performing model \n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " cast_to_float32 (CastToFloa  (None, 64, 64, 3)        0         \n",
      " t32)                                                            \n",
      "                                                                 \n",
      " normalization (Normalizatio  (None, 64, 64, 3)        7         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 30, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 30, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 64)        36928     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 512)       295424    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 512)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 13, 512)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 32)        147488    \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 9, 9, 32)          9248      \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 3591      \n",
      "                                                                 \n",
      " classification_head_1 (Soft  (None, 7)                0         \n",
      " max)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 512,078\n",
      "Trainable params: 512,071\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n",
      "None\n",
      "#Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodeltest\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodeltest\\assets\n"
     ]
    }
   ],
   "source": [
    "# START FIND MODEL \n",
    "\"\"\"\n",
    "Skin cancer lesion classification using the HAM10000 dataset\n",
    "Autokeras to find the best model. \n",
    "Dataset link:\n",
    "https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000\n",
    "Data description: \n",
    "https://arxiv.org/ftp/arxiv/papers/1803/1803.10417.pdf\n",
    "The 7 classes of skin cancer lesions included in this dataset are:\n",
    "Melanocytic nevi (nv)\n",
    "Melanoma (mel)\n",
    "Benign keratosis-like lesions (bkl)\n",
    "Basal cell carcinoma (bcc) \n",
    "Actinic keratoses (akiec)\n",
    "Vascular lesions (vas)\n",
    "Dermatofibroma (df)\n",
    "\n",
    "labels=['nv','mel','bkl','bcc','akiec','vas','df']\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "from keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import autokeras as ak\n",
    "\n",
    "skin_df = pd.read_csv('../backup/data/HAM10000/HAM10000_metadata.csv')\n",
    "\n",
    "\n",
    "SIZE=64\n",
    "\n",
    "# label encoding to numeric values from text\n",
    "le = LabelEncoder()\n",
    "le.fit(skin_df['dx'])\n",
    "LabelEncoder()\n",
    "#print(list(le.classes_))\n",
    " \n",
    "skin_df['label'] = le.transform(skin_df[\"dx\"]) \n",
    "#print(skin_df.sample(10))\n",
    "\n",
    "\n",
    "# Distribution of data into various classes \n",
    "from sklearn.utils import resample\n",
    "print(skin_df['label'].value_counts())\n",
    "\n",
    "#Balance data.\n",
    "# Many ways to balance data... you can also try assigning weights during model.fit\n",
    "#Separate each classes, resample, and combine back into single dataframe\n",
    "df=[]\n",
    "for i in range(7):\n",
    "    df.append( skin_df[skin_df['label'] == i])\n",
    "\n",
    "###############################################\n",
    "from sklearn.utils import shuffle\n",
    "# split 20% test 20% validation 60% train for every class\n",
    "df_test=[]\n",
    "df_validate=[]\n",
    "df_train=[]\n",
    "for i in range(7):\n",
    "    # shuffle the dataframe\n",
    "    df[i] = shuffle(df[i], random_state=42)\n",
    "    #split df in 20% as final testset\n",
    "    len_df=len(df[i])\n",
    "    len_test=int(len_df * 0.2)\n",
    "    len_validation=len_test\n",
    "    #len_rest=len_df - (len_test + len_validation)\n",
    "    df_test.append(df[i].head(len_test))\n",
    "    df[i]=df[i].tail(len_df - len_test)\n",
    "    df_validate.append(df[i].head(len_validation))\n",
    "    df_train.append(df[i].tail(len_df -len_validation -len_test))\n",
    "##########################################\n",
    "print('@$$$$$$$$$',df_train[0].shape,df_validate[0].shape,df_test[0].shape)\n",
    "#input()\n",
    "##########------------------###################################################################\n",
    "# resample train set classes to each 500 samples \n",
    "n_samples=500\n",
    "df_train_balanced=[]\n",
    "for i in range(7):\n",
    "    df_train_balanced.append( resample(df_train[i], replace=True, n_samples=n_samples, random_state=42) )\n",
    "print('@$$$-----$',df_train_balanced[0].shape,df_train[0].shape,df_validate[0].shape,df_test[0].shape)\n",
    "#input()\n",
    "print('#prepare train set')\n",
    "# prepare train set\n",
    "# #########-------------###########################\n",
    "def prepare_set(df_set,skin_df = skin_df):\n",
    "    print(type(df_set))\n",
    "    \n",
    "    #put all classes together\n",
    "    df_set = pd.concat([df_set[0], df_set[1], df_set[2], df_set[3], df_set[4], df_set[5], df_set[6]])\n",
    "    #read images using image ID from the CSV file\n",
    "    image_path = {os.path.splitext(os.path.basename(x))[0]: x\n",
    "                        for x in glob(os.path.join('../backup/data/HAM10000/HAM10000_images_part_?', '*.jpg'))}\n",
    "    #Define the path and add as a new column\n",
    "    df_set['path'] = skin_df['image_id'].map(image_path.get)\n",
    "    #Use the path to read images.\n",
    "    df_set['image'] = df_set['path'].map(lambda x: np.asarray(Image.open(x).resize((SIZE,SIZE))))\n",
    "    #Convert dataframe column of images into numpy array\n",
    "    X = np.asarray(df_set['image'].tolist())\n",
    "    X = X/255. # Scale values to 0-1. You can also used standardscaler or other scaling methods.\n",
    "    Y=df_set['label'] #Assign label values to Y\n",
    "    Y_cat = to_categorical(Y, num_classes=7) #Convert to categorical -> multiclass classification \n",
    "    return X, Y_cat\n",
    "    #########-------------------------######################################################\n",
    "# prepare all sets to be used\n",
    "X_train_balanced,Y_cat_train_balanced = prepare_set(df_train_balanced)\n",
    "X_test,Y_cat_test = prepare_set(df_test)\n",
    "X_validate,Y_cat_validate = prepare_set(df_validate)\n",
    "\n",
    "print('@$$$--x---$',X_test.shape,X_train_balanced.shape,X_validate.shape,'@@', df_train_balanced[0].shape,df_train[0].shape,df_validate[0].shape,df_test[0].shape)\n",
    "#input()\n",
    "\n",
    "print('@@@@@@@',len(X_train_balanced),len(X_test),len(X_validate))\n",
    "# for selecting the best model we will use the validation set and split this into a train/test\n",
    "#\n",
    "# start model selection\n",
    "print('# start model selection')\n",
    "x_train_auto, x_test_auto, y_train_auto, y_test_auto = train_test_split(X_validate, Y_cat_validate, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define classifier for autokeras. Here we check 25 different models, each model 25 epochs\n",
    "# \n",
    "clf = ak.ImageClassifier(max_trials=25) #MaxTrials - max. number of keras models to try\n",
    "clf.fit(x_train_auto, y_train_auto, epochs=25)\n",
    "\n",
    "#Evaluate the classifier on test data\n",
    "print('#Evaluate the classifier on test data ')\n",
    "\n",
    "_, acc = clf.evaluate(x_test_auto, y_test_auto)\n",
    "print(\"Accuracy = \", (acc * 100.0), \"%\")\n",
    "\n",
    "# get the final best performing model\n",
    "print('# get the final best performing model ')\n",
    "\n",
    "model = clf.export_model()\n",
    "print(model.summary())\n",
    "print('#Save the model')\n",
    "'#Save the model'\n",
    "model.save('../pybin/models/activemodeltest')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodel_64\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodel_64\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 75s 675ms/step - loss: 1.6012 - accuracy: 0.3629\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 74s 675ms/step - loss: 1.3664 - accuracy: 0.4549\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 74s 675ms/step - loss: 1.2685 - accuracy: 0.5069\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 78s 711ms/step - loss: 1.1739 - accuracy: 0.5431\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 73s 666ms/step - loss: 1.1348 - accuracy: 0.5663\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 74s 670ms/step - loss: 1.0351 - accuracy: 0.6029\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 74s 671ms/step - loss: 1.0293 - accuracy: 0.6186\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 73s 664ms/step - loss: 0.9661 - accuracy: 0.6386\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 73s 664ms/step - loss: 0.9219 - accuracy: 0.6517\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 72s 658ms/step - loss: 0.8587 - accuracy: 0.6726\n",
      "63/63 [==============================] - 10s 152ms/step - loss: 1.0494 - accuracy: 0.5860\n",
      "[1.0494091510772705, 0.5860000252723694]\n",
      "#Save the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodel_64_trained\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../pybin/models/activemodel_64_trained\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#train model\n",
    "#X_train_balanced,Y_cat_train_balanced = prepare_set(df_train_balanced)\n",
    "#X_test,Y_cat_test = prepare_set(df_test)\n",
    "#X_validate,Y_cat_validate = prepare_set(df_validate)\n",
    "model.save('../pybin/models/activemodel_64')\n",
    "\n",
    "model = keras.models.load_model('../pybin/models/activemodeltest')\n",
    "\n",
    "model.fit(X_train_balanced,Y_cat_train_balanced,epochs=10)\n",
    "res=model.evaluate(X_test,Y_cat_test)\n",
    "print(res)\n",
    "print('#Save the model')\n",
    "'#Save the model'\n",
    "model.save('../pybin/models/activemodel_64_trained')\n",
    "\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad39fc4de9d0ed75de8360435f22c5c59caf607eb9db992e2697dd87e09bea1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
